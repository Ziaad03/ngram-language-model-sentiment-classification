{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VAURKtoNlAuV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import Counter\n",
        "from math import log, exp\n",
        "import requests\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_imdb_unsup_sentences(folder_path):\n",
        "    \"\"\"\n",
        "    Loads text files from the IMDB 'unsup' (unsupervised) folder.\n",
        "    split text by newline, strips text, and returns a list of raw lines.\n",
        "    replace <br /> tags with special token <nl> token.\n",
        "    \"\"\"\n",
        "\n",
        "    all_sentences = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(\n",
        "                os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\"\n",
        "            ) as file:\n",
        "                for line in file:\n",
        "                    line = line.strip().replace(\"<br />\", \" <nl> \")\n",
        "                    all_sentences.append(line)\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"\n",
        "    Removes punctuation from the text,\n",
        "    but keeps <nl> tokens intact.\n",
        "    \"\"\"\n",
        "    text = re.sub(\n",
        "        r\"(?<!<nl>)[{}]+(?!<nl>)\".format(re.escape(string.punctuation)), \"\", text\n",
        "    )\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def build_vocabulary(sentences):\n",
        "    \"\"\"\n",
        "    lower each sentence,\n",
        "    Splits each sentence on whitespace, removes punctuation,\n",
        "    and builds a set of unique tokens (vocabulary).\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = remove_punctuation(sentence)\n",
        "        tokens = sentence.split()\n",
        "        vocab.update(tokens)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def tokenize(sentences, vocab, unknown=\"<UNK>\"):\n",
        "    \"\"\"\n",
        "    lower each sentence,\n",
        "    Splits each sentence on whitespace, removes punctuation,\n",
        "    and replaces tokens not in the vocabulary with unknown token.\n",
        "    Returns the list of tokenized sentences.\n",
        "    \"\"\"\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = remove_punctuation(sentence)\n",
        "        tokens = sentence.split()\n",
        "        tokenized_sentence = [token if token in vocab else unknown for token in tokens]\n",
        "        tokenized_sentences.append(tokenized_sentence)\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "\n",
        "def split_data(sentences, test_split=0.1):\n",
        "    \"\"\"\n",
        "    shuffle the sentences\n",
        "    split them into train and test sets (first 1-test_split of the data is the training)\n",
        "    return the train and test sets\n",
        "    \"\"\"\n",
        "    random.shuffle(sentences)\n",
        "    split_index = int(len(sentences) * (1 - test_split))\n",
        "    train_sentences = sentences[:split_index]\n",
        "    test_sentences = sentences[split_index:]\n",
        "\n",
        "    return train_sentences, test_sentences\n",
        "\n",
        "\n",
        "def pad_sentence(tokens, n):\n",
        "    \"\"\"\n",
        "    Pads a list of tokens with <s> at the start (n-1 times)\n",
        "    and </s> at the end (once).\n",
        "    For example, if n=3, you add 2 <s> tokens at the start.\n",
        "    \"\"\"\n",
        "    padded = [\"<s>\"] * (n - 1) + tokens + [\"</s>\"]\n",
        "    return padded\n",
        "\n",
        "\n",
        "def build_ngram_counts(tokenized_sentences, n):\n",
        "    \"\"\"\n",
        "    Builds n-gram counts and (n-1)-gram counts from the given tokenized sentences.\n",
        "    Each sentence is padded with <s> and </s>.\n",
        "\n",
        "    Args:\n",
        "        tokenized_sentences: list of lists, where each sub-list is a tokenized sentence.\n",
        "        n: the order of the n-gram (e.g., 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "        ngram_counts: Counter of n-grams (tuples of length n).\n",
        "        context_counts: Counter of (n-1)-gram contexts.\n",
        "    \"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i : i + n])\n",
        "            context = tuple(padded_sentence[i : i + n - 1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "\n",
        "def laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the probability of an n-gram using Laplace (add-alpha) smoothing.\n",
        "\n",
        "    P(w_i | w_{i-(n-1)}, ..., w_{i-1}) =\n",
        "        (count(ngram) + alpha) / (count(context) + alpha * vocab_size)\n",
        "\n",
        "    Args:\n",
        "        ngram: tuple of tokens representing the n-gram\n",
        "        ngram_counts: Counter of n-grams\n",
        "        context_counts: Counter of (n-1)-gram contexts\n",
        "        vocab_size: size of the vocabulary\n",
        "        alpha: smoothing parameter (1.0 = add-1 smoothing)\n",
        "\n",
        "    Returns:\n",
        "        Probability of the given n-gram.\n",
        "    \"\"\"\n",
        "    ngram_count = ngram_counts[ngram]\n",
        "    context = ngram[:-1]\n",
        "    context_count = context_counts[context]\n",
        "    prob = (ngram_count + alpha) / (context_count + alpha * vocab_size)\n",
        "    return prob\n",
        "\n",
        "\n",
        "def predict_next_token(\n",
        "    context_tokens, ngram_counts, context_counts, vocab, n=2, alpha=1.0, top_k=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Given a list of context tokens, predict the next token using the n-gram model.\n",
        "    Returns the top_k predictions as (token, probability).\n",
        "    \"\"\"\n",
        "    context = tuple(context_tokens[-(n - 1) :])\n",
        "    candidates = []\n",
        "\n",
        "    for token in vocab:\n",
        "        ngram = context + (token,)\n",
        "        prob = laplace_probability(\n",
        "            ngram, ngram_counts, context_counts, len(vocab), alpha\n",
        "        )\n",
        "        candidates.append((token, prob))\n",
        "\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:top_k]\n",
        "\n",
        "\n",
        "def generate_text_with_limit(\n",
        "    start_tokens, ngram_counts, context_counts, vocab, n=2, alpha=1.0, max_length=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates text from an n-gram model until it sees </s>\n",
        "    or reaches a maximum total length (max_length).\n",
        "\n",
        "    Args:\n",
        "        start_tokens (list): initial context to begin generation\n",
        "        ngram_counts (Counter): trained n-gram counts\n",
        "        context_counts (Counter): trained (n-1)-gram counts\n",
        "        vocab (set): the model vocabulary\n",
        "        n (int): n-gram order, 2 for bigram, 3 for trigram, etc.\n",
        "        alpha (float): Laplace smoothing parameter\n",
        "        max_length (int): maximum number of tokens to generate (including start_tokens)\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens representing the generated sequence.\n",
        "    \"\"\"\n",
        "    generated = start_tokens[:]\n",
        "\n",
        "    while len(generated) < max_length:\n",
        "        context_tokens = generated[-(n - 1) :]\n",
        "        next_token_candidates = predict_next_token(\n",
        "            context_tokens,\n",
        "            ngram_counts,\n",
        "            context_counts,\n",
        "            vocab,\n",
        "            n,\n",
        "            alpha,\n",
        "            top_k=10,  # Increased top_k\n",
        "        )\n",
        "\n",
        "        if not next_token_candidates:\n",
        "            break\n",
        "\n",
        "        # Use weighted random choice based on probabilities\n",
        "        total_prob = sum(prob for _, prob in next_token_candidates)\n",
        "        rand_val = random.random() * total_prob\n",
        "        cumulative = 0\n",
        "\n",
        "        for token, prob in next_token_candidates:\n",
        "            cumulative += prob\n",
        "            if cumulative > rand_val:\n",
        "                next_token = token\n",
        "                break\n",
        "        else:\n",
        "            next_token = next_token_candidates[0][0]\n",
        "\n",
        "        if next_token == \"</s>\":\n",
        "            break\n",
        "\n",
        "        generated.append(next_token)\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "def calculate_perplexity(\n",
        "    tokenized_sentences, ngram_counts, context_counts, vocab_size, n=2, alpha=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of an n-gram model (with Laplace smoothing)\n",
        "    on a list of tokenized sentences.\n",
        "\n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of tokens.\n",
        "        ngram_counts: Counter of n-grams.\n",
        "        context_counts: Counter of (n-1)-grams.\n",
        "        vocab_size: Size of the vocabulary.\n",
        "        n: n-gram order.\n",
        "        alpha: Laplace smoothing parameter.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the perplexity on the given dataset.\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i : i + n])\n",
        "            prob = laplace_probability(\n",
        "                ngram, ngram_counts, context_counts, vocab_size, alpha\n",
        "            )\n",
        "            log_prob_sum += log(prob)\n",
        "            total_tokens += 1\n",
        "\n",
        "    perplexity = exp(-log_prob_sum / total_tokens)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "WH5RkNRUlKoi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the IMDB dataset\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "output_path = \"/content/aclImdb_v1.tar.gz\"\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(output_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    raise Exception(\"Failed to download the dataset\")\n",
        "\n",
        "# Extract the tar.gz file\n",
        "with tarfile.open(output_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=\"/content\")\n",
        "print(\"Extraction complete!\")\n",
        "\n",
        "# Set the path to the unsupervised folder\n",
        "imdb_folder = \"/content/aclImdb/train/unsup\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZkfTT-IlLbn",
        "outputId": "a59f690c-c1fa-409d-fc9f-a512d92b88de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = load_imdb_unsup_sentences(imdb_folder)\n",
        "\n",
        "assert len(sentences) == 50000, \"Expected 50,000 sentences from the unsup folder.\"\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "train_sentences, test_sentences = split_data(sentences)\n",
        "\n",
        "\n",
        "assert len(train_sentences) == 45000, \"Expected 45,000 sentences for training.\"\n",
        "assert len(test_sentences) == 5000, \"Expected 5,000 sentences for testing.\""
      ],
      "metadata": {
        "id": "K0FXuiIFo_s6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocabulary(train_sentences)\n",
        "tokenized_sentences = tokenize(train_sentences, vocab)\n",
        "\n",
        "assert (\n",
        "    len(tokenized_sentences) == 45000\n",
        "), \"Expected tokenized sentences count to match raw sentences.\"\n",
        "\n",
        "example = \"I love Natural language processing, and i want to be a great engineer.\"\n",
        "assert (\n",
        "    len(example) == 70\n",
        "), \"Example sentence length (in characters) does not match the expected 70.\"\n",
        "\n",
        "example_tokens = tokenize([example], vocab)[0]\n",
        "assert (\n",
        "    len(example_tokens) == 13\n",
        "), \"Token count for the example sentence does not match the expected 13.\"\n"
      ],
      "metadata": {
        "id": "oFqSU_ZjpJ8p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building N-gram Models\n",
        "### For alpha = 0.5"
      ],
      "metadata": {
        "id": "oKccgmAIrgdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2\n",
        "alpha = 0.5\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=alpha,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n",
        "\n",
        "test_tockenized_sentences = tokenize(test_sentences, vocab)\n",
        "print(\n",
        "    f\"Preplexity: {calculate_perplexity(test_tockenized_sentences, ngram_counts, context_counts, len(vocab), n, alpha)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktkVHalkqXPB",
        "outputId": "71bcecfc-b375-45bf-c881-cb131a296e9a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'with', 'a', 'movie', 'the', 'movie', 'is', 'the', 'film', 'i', 'saw', 'this', 'is', 'just', 'a', 'very', 'well', 'and', 'i', 'was', 'not', 'to', 'watch', 'a', 'bit', 'too', 'long', 'as', 'a', 'lot', 'of', 'the', 'first', 'one', 'to', 'get', 'to', 'the', 'plot', 'is', 'the', 'first', 'of', 'his', 'father', 'has', 'been', 'more', 'to', 'the', 'most', 'of', 'the', 'movie', 'and', 'the', 'first', 'two', 'of', 'the', 'only', 'one', 'thing', 'but', 'it', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'nl', 'and', 'the', 'plot', 'is', 'a', 'movie', 'that', 'is', 'a', 'great', 'as', 'he', 'has', 'been', 'better', 'than', 'any', 'other', 'than', 'anything', 'but', 'it', 'and', 'he', 'has', 'to', 'be', 'one', 'of', 'a', 'great', 'film', 'was', 'so', 'i', 'dont', 'like', 'this', 'film', 'i', 'saw', 'in', 'the', 'movie', 'nl', 'nl', 'nl', 'nl', 'it', 'to', 'have', 'been']\n",
            "Preplexity: 2385.220921090936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "\n",
        "  * **Coherence**:\n",
        "Starts reasonably with “i love with a movie the movie is the film,” which is somewhat grammatical but awkward. It then drifts into a mix of semi-coherent phrases (“i saw this is just a very well”) and fragmented ideas (“the plot is the first of his father has been”).\n",
        "  * **Repetition**: Repeats common words like “the” (14 times), “a” (8 times), “movie” (5 times), and “nl” (10 times in a row). No single word dominates excessively.\n",
        "  * **Special Tokens**: The `<nl>` token appears frequently, especially in a long stretch, suggesting the model learned it as a common continuation in the IMDB data (reviews often have line breaks).\n",
        "\n",
        "#### Interpretation:\n",
        "With `n=2`, the context is just the last token (e.g., \"love\" → \"with\"), making it prone to generic transitions. `alpha=0.5` (light smoothing) favors frequent bigrams, leading to a mix of movie-related words (“movie,” “film,” “plot”) and connectors (“the,” “a”), but it struggles to maintain long-term coherence."
      ],
      "metadata": {
        "id": "z1Ked-1GxYYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "alpha = 0.5\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=alpha,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n",
        "\n",
        "test_tockenized_sentences = tokenize(test_sentences, vocab)\n",
        "print(\n",
        "    f\"Preplexity: {calculate_perplexity(test_tockenized_sentences, ngram_counts, context_counts, len(vocab), n, alpha)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxC_oDr6rR3i",
        "outputId": "2d488c67-3e49-43ba-be22-c6627ef99506"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'the', 'movie', 'is', 'a', 'great', 'film', 'but', 'i', 'think', 'this', 'film', 'has', 'to', 'be', 'an', 'action', 'movie', 'in', 'fact', 'this', 'film', 'is', 'not', 'the', 'best', 'part', 'of', 'the', 'most', 'part', 'the', 'film', 'and', 'it', 'was', 'a', 'bit', 'of', 'fun', 'but', 'he', 'was', 'so', 'awful', 'its', 'actually', 'a', 'scene', 'that', 'was', 'a', 'very', 'young', 'kids', 'might', 'enjoy', 'it', 'for', 'me', 'is', 'the', 'only', 'thing', 'the', 'story', 'is', 'about', 'a', 'young', 'age', 'i', 'was', 'a', 'kid', 'and', 'the', 'acting', 'is', 'pretty', 'much', 'just', 'seeing', 'daffy', 'ghostsand', 'lifesent', 'comebackcare', 'shadowcat', 'lifesent', 'handto', 'imrie', 'cinémavérité', 'lifesent', 'sepiatoned', 'cinémavérité', 'cinémavérité', 'jot', 'jot', 'jot', 'cinémavérité', 'comebackcare', 'imrie', 'sepiatoned', 'jot', 'handto', 'lifesent', 'sepiatoned', 'comebackcare', 'sepiatoned', 'phones', 'comebackcare', 'handto', 'ghostsand', 'shadowcat', 'handto', 'shadowcat', 'imrie', 'handto', 'sepiatoned', 'ghostsand', 'shadowcat', 'comebackcare', 'cinémavérité', 'shadowcat', 'jot', 'jot', 'handto']\n",
            "Preplexity: 28116.45614560796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "\n",
        "* **Coherence**: Much better initially: “i love the movie is a great film but i think this film has to be an action movie” reads like a natural review snippet. It stays coherent through “kids might enjoy it for me is the only thing,” then degrades into gibberish with rare tokens like “ghostsand,” “cinémavérité,” and “imrie.”\n",
        "* **Repetition**: Less repetition of common words (“the” 8 times, “film” 4 times), but a late stretch repeats obscure tokens (“cinémavérité,” “jot,” “shadowcat”) multiple times.\n",
        "* **Special Tokens**: No `<nl>`, but rare words dominate the end, possibly low-frequency tokens from the training data.\n",
        "\n",
        "#### Interpretation:\n",
        "`n=3` uses two-token contexts (e.g., \"i love\" → \"the\"), capturing more structure than bigrams. `alpha=0.5` keeps probabilities tied to counts, so early coherence reflects frequent trigrams, but sparsity later (unseen contexts) lets smoothing favor rare tokens."
      ],
      "metadata": {
        "id": "uOJAoVXgx_4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "alpha = 0.5\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=alpha,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n",
        "\n",
        "test_tockenized_sentences = tokenize(test_sentences, vocab)\n",
        "print(\n",
        "    f\"Preplexity: {calculate_perplexity(test_tockenized_sentences, ngram_counts, context_counts, len(vocab), n, alpha)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzOey3G9r2bx",
        "outputId": "d2ae3b27-9634-410d-a53f-3f387cc6e1e1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'imrie', 'imrie', 'phones', 'comebackcare', 'jot', 'imrie', 'imrie', 'lifesent', 'sepiatoned', 'jot', 'shadowcat', 'comebackcare', 'cinémavérité', 'shadowcat', 'comebackcare', 'jot', 'ghostsand', 'imrie', 'handto', 'handto', 'sepiatoned', 'imrie', 'handto', 'imrie', 'phones', 'handto', 'cinémavérité', 'imrie', 'ghostsand', 'cinémavérité', 'shadowcat', 'imrie', 'handto', 'shadowcat', 'comebackcare', 'lifesent', 'lifesent', 'ghostsand', 'handto', 'imrie', 'handto', 'cinémavérité', 'jot', 'handto', 'cinémavérité', 'handto', 'jot', 'ghostsand', 'handto', 'shadowcat', 'imrie', 'handto', 'jot', 'cinémavérité', 'shadowcat', 'lifesent', 'lifesent', 'ghostsand', 'cinémavérité', 'cinémavérité', 'shadowcat', 'jot', 'lifesent', 'shadowcat', 'phones', 'cinémavérité', 'jot', 'jot', 'ghostsand', 'handto', 'phones', 'handto', 'ghostsand', 'handto', 'ghostsand', 'lifesent', 'jot', 'handto', 'imrie', 'imrie', 'imrie', 'cinémavérité', 'shadowcat', 'sepiatoned', 'shadowcat', 'phones', 'sepiatoned', 'sepiatoned', 'shadowcat', 'jot', 'jot', 'lifesent', 'handto', 'ghostsand', 'comebackcare', 'imrie', 'shadowcat', 'comebackcare', 'lifesent', 'shadowcat', 'shadowcat', 'lifesent', 'imrie', 'ghostsand', 'sepiatoned', 'shadowcat', 'comebackcare', 'cinémavérité', 'lifesent', 'shadowcat', 'shadowcat', 'phones', 'imrie', 'ghostsand', 'shadowcat', 'cinémavérité', 'comebackcare', 'comebackcare', 'imrie', 'imrie', 'cinémavérité', 'cinémavérité', 'sepiatoned', 'cinémavérité', 'comebackcare', 'lifesent']\n",
            "Preplexity: 90245.60548184061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "\n",
        "* **Coherence**: Starts with “i love” then immediately becomes incoherent with “imrie imrie phones.” It’s a jumble of rare tokens like “imrie,” “cinémavérité,” “shadowcat,” and “jot.”\n",
        "* **Repetition**: High repetition of specific rare words: “imrie” (12 times), “handto” (13 times), “cinémavérité” (11 times), “shadowcat” (11 times), “jot” (10 times).\n",
        "* **Special Tokens**: No `<nl>`, just a flood of obscure tokens.\n",
        "\n",
        "#### Interpretation:\n",
        "`n=4` uses three-token contexts (e.g., \"i love imrie\" → \"imrie\"), but the training data likely has sparse four-grams. With `alpha=0.5`, unseen contexts get low probabilities, and smoothing amplifies rare tokens that appear in a few contexts, causing a feedback loop of repetition."
      ],
      "metadata": {
        "id": "eiOJ3EBmyVMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For alpha = 1\n"
      ],
      "metadata": {
        "id": "S0g0fpk1r-Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2\n",
        "alpha = 1\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=alpha,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n",
        "\n",
        "test_tockenized_sentences = tokenize(test_sentences, vocab)\n",
        "print(\n",
        "    f\"Preplexity: {calculate_perplexity(test_tockenized_sentences, ngram_counts, context_counts, len(vocab), n, alpha)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAah5K97pP7e",
        "outputId": "165ba024-fa8f-4157-df07-8b6397a55d74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'with', 'an', 'interesting', 'and', 'i', 'am', 'not', 'a', 'film', 'nl', 'in', 'a', 'bit', 'as', 'well', 'it', 'was', 'the', 'first', 'one', 'of', 'a', 'great', 'job', 'of', 'the', 'most', 'people', 'who', 'has', 'an', 'excellent', 'nl', 'the', 'movie', 'with', 'the', 'film', 'and', 'the', 'movie', 'the', 'movie', 'and', 'the', 'movie', 'that', 'he', 'is', 'just', 'as', 'well', 'nl', 'nl', 'nl', 'nl', 'nl', 'i', 'can', 'make', 'up', 'in', 'the', 'story', 'about', 'the', 'first', 'saw', 'the', 'story', 'and', 'he', 'can', 'be', 'a', 'great', 'film', 'the', 'story', 'line', 'of', 'the', 'best', 'performance', 'as', 'a', 'few', 'years', 'ago', 'and', 'the', 'movie', 'i', 'was', 'one', 'is', 'an', 'interesting', 'and', 'the', 'same', 'time', 'with', 'a', 'movie', 'with', 'an', 'actor', 'he', 'did', 'not', 'the', 'only', 'thing', 'that', 'was', 'so', 'i', 'was', 'a', 'good', 'and', 'the', 'film', 'is', 'that']\n",
            "Preplexity: 3473.804355257891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "\n",
        "* **Coherence**: Starts with “i love with an interesting and i am not a film,” which is odd but somewhat readable. Phrases like “the movie with the film and the movie” and “he can be a great film the story line” show partial coherence but lack fluency.\n",
        "* **Repetition**: “the” (15 times), “movie” (6 times), “nl” (5 times), “a” (8 times). No extreme single-word repetition.\n",
        "* **Special Tokens**: <nl> appears in bursts, reflecting its frequency in reviews.\n",
        "\n",
        "#### Interpretation:\n",
        "`alpha=1.0` (standard Laplace smoothing) balances counts and smoothing more than `0.5`, producing slightly more varied bigrams. Still, the short context limits coherence."
      ],
      "metadata": {
        "id": "yocDnzc9ynBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "alpha = 1\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=alpha,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n",
        "\n",
        "test_tockenized_sentences = tokenize(test_sentences, vocab)\n",
        "print(\n",
        "    f\"Preplexity: {calculate_perplexity(test_tockenized_sentences, ngram_counts, context_counts, len(vocab), n, alpha)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvDyyQ5Cq8Cd",
        "outputId": "6608032a-f26f-49d2-c63d-5bf1ff3c6ad3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'this', 'show', 'is', 'the', 'best', 'thing', 'about', 'this', 'movie', 'is', 'not', 'the', 'case', 'with', 'so', 'much', 'for', 'the', 'rest', 'of', 'the', 'movie', 'was', 'very', 'well', 'but', 'the', 'film', 'and', 'a', 'half', 'of', 'the', 'movie', 'was', 'a', 'good', 'job', 'with', 'the', 'characters', 'are', 'all', 'the', 'way', 'they', 'stop', 'being', 'funny', 'as', 'the', 'film', 'that', 'doesnt', 'matter', 'how', 'much', 'of', 'an', 'old', 'fashioned', 'horror', 'mmovie', 'ghostsand', 'ghostsand', 'handto', 'imrie', 'cinémavérité', 'ghostsand', 'shadowcat', 'jot', 'comebackcare', 'lifesent', 'imrie', 'lifesent', 'lifesent', 'handto', 'handto', 'shadowcat', 'ghostsand', 'comebackcare', 'jot', 'lifesent', 'comebackcare', 'ghostsand', 'lifesent', 'imrie', 'sepiatoned', 'lifesent', 'shadowcat', 'ghostsand', 'jot', 'ghostsand', 'ghostsand', 'lifesent', 'shadowcat', 'handto', 'sepiatoned', 'shadowcat', 'cinémavérité', 'cinémavérité', 'imrie', 'ghostsand', 'imrie', 'handto', 'phones', 'cinémavérité', 'imrie', 'jot', 'handto', 'comebackcare', 'handto', 'sepiatoned', 'sepiatoned', 'jot', 'imrie', 'imrie', 'sepiatoned', 'handto', 'jot', 'handto', 'comebackcare', 'cinémavérité', 'lifesent', 'comebackcare', 'phones']\n",
            "Preplexity: 37624.88649906199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "\n",
        "* **Coherence**: Strong start: “i love this show is the best thing about this movie is not the case with so much for the rest of the movie” feels like a review. It holds up through “old fashioned horror mmovie,” then collapses into rare tokens.\n",
        "* **Repetition**: “the” (11 times), “movie” (3 times), later “ghostsand” (6 times), “imrie” (5 times).\n",
        "* **Special Tokens**: No <nl>, but rare tokens take over later.\n",
        "\n",
        "#### Interpretation:\n",
        "`alpha=1.0` with `n=3` improves early coherence by smoothing more, but sparsity in trigrams still leads to rare-token dominance later."
      ],
      "metadata": {
        "id": "xanu32HF16l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "alpha = 1\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=n,\n",
        "    alpha=alpha,\n",
        "    max_length=128,\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", generated_seq)\n",
        "\n",
        "test_tockenized_sentences = tokenize(test_sentences, vocab)\n",
        "print(\n",
        "    f\"Preplexity: {calculate_perplexity(test_tockenized_sentences, ngram_counts, context_counts, len(vocab), n, alpha)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmut-qQBsZ_J",
        "outputId": "509ebf5e-028e-4de1-c0f8-bc92a29a749d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'handto', 'phones', 'shadowcat', 'comebackcare', 'jot', 'comebackcare', 'lifesent', 'lifesent', 'phones', 'ghostsand', 'comebackcare', 'lifesent', 'jot', 'lifesent', 'lifesent', 'handto', 'sepiatoned', 'jot', 'ghostsand', 'shadowcat', 'ghostsand', 'jot', 'shadowcat', 'lifesent', 'jot', 'jot', 'shadowcat', 'comebackcare', 'shadowcat', 'phones', 'handto', 'jot', 'phones', 'lifesent', 'handto', 'lifesent', 'sepiatoned', 'ghostsand', 'jot', 'lifesent', 'shadowcat', 'jot', 'handto', 'ghostsand', 'sepiatoned', 'cinémavérité', 'lifesent', 'lifesent', 'lifesent', 'shadowcat', 'jot', 'imrie', 'imrie', 'comebackcare', 'jot', 'jot', 'ghostsand', 'lifesent', 'shadowcat', 'cinémavérité', 'cinémavérité', 'sepiatoned', 'shadowcat', 'shadowcat', 'comebackcare', 'comebackcare', 'cinémavérité', 'sepiatoned', 'sepiatoned', 'jot', 'shadowcat', 'comebackcare', 'lifesent', 'shadowcat', 'imrie', 'phones', 'comebackcare', 'imrie', 'comebackcare', 'shadowcat', 'shadowcat', 'ghostsand', 'jot', 'handto', 'ghostsand', 'sepiatoned', 'imrie', 'cinémavérité', 'jot', 'handto', 'phones', 'lifesent', 'sepiatoned', 'cinémavérité', 'phones', 'ghostsand', 'ghostsand', 'shadowcat', 'imrie', 'handto', 'jot', 'imrie', 'lifesent', 'comebackcare', 'jot', 'cinémavérité', 'phones', 'imrie', 'comebackcare', 'sepiatoned', 'handto', 'ghostsand', 'cinémavérité', 'sepiatoned', 'shadowcat', 'handto', 'comebackcare', 'sepiatoned', 'cinémavérité', 'handto', 'shadowcat', 'handto', 'ghostsand', 'ghostsand', 'sepiatoned', 'imrie']\n",
            "Preplexity: 102113.1509005188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "\n",
        "* **Coherence**: Starts “i love handto phones” and stays incoherent, dominated by rare tokens.\n",
        "* **Repetition**: “lifesent” (12 times), “jot” (11 times), “shadowcat” (10 times), “cinémavérité” (8 times).\n",
        "* **Special Tokens**: No <nl>, just rare-token chaos.\n",
        "\n",
        "#### Interpretation:\n",
        " `alpha=1.0` smooths more, but `n=4` sparsity makes it worse, amplifying rare tokens in a repetitive loop."
      ],
      "metadata": {
        "id": "jO9olPCS2WvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Notes\n",
        "#### 1. Trend with `n`:\n",
        "Perplexity increases dramatically as `n` grows (2 → 3 → 4) for both alpha values. This reflects data sparsity—higher-order n-grams (trigrams, four-grams) have fewer occurrences, making the model less predictive on test data.\n",
        "#### 2. Trend with `alpha`:\n",
        " For each n, `alpha=1.0` yields higher perplexity than alpha=`0.5`. Lighter smoothing (`0.5`) relies more on counts, fitting the training data better, while `1.0` smooths more, increasing probabilities for unseen n-grams and raising perplexity.\n",
        "#### 3. Best Model:\n",
        " `n=2`, `alpha=0.5` has the lowest perplexity (2385.22), suggesting bigrams with light smoothing generalize best to the test set."
      ],
      "metadata": {
        "id": "RJM8kIZ927Hh"
      }
    }
  ]
}